{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['negative_tweets.json', 'positive_tweets.json', 'tweets.20150430-223406.json']\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "\n",
    "from nltk.corpus import twitter_samples\n",
    "from nltk.tokenize import word_tokenize,sent_tokenize\n",
    "\n",
    "import string\n",
    "import re\n",
    " \n",
    "from nltk.corpus import stopwords \n",
    "stopwords_english = stopwords.words('english')\n",
    " \n",
    "from nltk.stem import WordNetLemmatizer\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "\n",
    "print (twitter_samples.fileids())\n",
    "\n",
    "\n",
    "pos_tweets = twitter_samples.strings('positive_tweets.json')\n",
    "\n",
    "neg_tweets = twitter_samples.strings('negative_tweets.json')\n",
    "\n",
    " \n",
    "all_tweets = twitter_samples.strings('tweets.20150430-223406.json')\n",
    "# Happy Emotions\n",
    "emotions_happy = set([\n",
    "    ':-)', ':)', ';)', ':o)', ':]', ':3', ':c)', ':>', '=]', '8)', '=)', ':}',\n",
    "    ':^)', ':-D', ':D', '8-D', '8D', 'x-D', 'xD', 'X-D', 'XD', '=-D', '=D',\n",
    "    '=-3', '=3', ':-))', \":'-)\", \":')\", ':*', ':^*', '>:P', ':-P', ':P', 'X-P',\n",
    "    'x-p', 'xp', 'XP', ':-p', ':p', '=p', ':-b', ':b', '>:)', '>;)', '>:-)',\n",
    "    '<3'\n",
    "    ])\n",
    " \n",
    "# Sad Emotions\n",
    "emotions_sad = set([\n",
    "    ':L', ':-/', '>:/', ':S', '>:[', ':@', ':-(', ':[', ':-||', '=L', ':<',\n",
    "    ':-[', ':-<', '=\\\\', '=/', '>:(', ':(', '>.<', \":'-(\", \":'(\", ':\\\\', ':-c',\n",
    "    ':c', ':{', '>:\\\\', ';('\n",
    "    ])\n",
    "\n",
    "emotions = emotions_happy.union(emotions_sad)\n",
    "\n",
    "\n",
    "def clean_tweets(tweet):\n",
    "    \n",
    "    # remove  retweet text \"RT\"\n",
    "    tweet = re.sub(r'^RT[\\s]+', '', tweet)\n",
    "    #remove new lines\n",
    "    tweet = re.sub('\\n',' ',tweet)\n",
    "     \n",
    "    # remove urls\n",
    "    tweet = re.sub(r'https?:\\/\\/.*[\\r\\n]*|www.*?\\s', '', tweet)\n",
    "    \n",
    "    # remove html characters\n",
    "    tweet = re.sub(r'#|%|&|—', '', tweet)\n",
    " \n",
    "    #tweet_tokens = word_tokenize(tweet)\n",
    "    tokenizer = TweetTokenizer(preserve_case=False, strip_handles=True, reduce_len=True)\n",
    "    tweet_tokens = tokenizer.tokenize(tweet)\n",
    " \n",
    " \n",
    "    new_tweets = []\n",
    "    \n",
    "    for word in tweet_tokens:\n",
    "        if (word not in stopwords_english and  # remove stop words\n",
    "                word not in string.punctuation and word not in emotions ): # remove punctuation and emotions\n",
    "            #tweets lemmatization\n",
    "            #new_tweets.append(lemmatizer.lemmatize(word.lower(),pos=\"v\"))\n",
    "            new_tweets.append(word)\n",
    "\n",
    " \n",
    "    return new_tweets\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Hello I need to know something can u fm me on Twitter?? — sure thing :) dm me x http://t.co/W6Dy130BV7\n",
      "['hello', 'need', 'know', 'something', 'u', 'fm', 'twitter', 'sure', 'thing', 'dm', 'x']\n"
     ]
    }
   ],
   "source": [
    "print (pos_tweets[19])\n",
    "print ((clean_tweets(pos_tweets[19])))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('lay', 'NN'), ('greet', 'NN'), ('card', 'NN'), ('range', 'NN'), ('print', 'NN'), ('today', 'NN'), ('love', 'VBP'), ('job', 'NN')]\n"
     ]
    }
   ],
   "source": [
    "print (nltk.pos_tag(clean_tweets(pos_tweets[15])))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['like',\n",
       " 'keep',\n",
       " 'lovely',\n",
       " 'customers',\n",
       " 'waiting',\n",
       " 'long',\n",
       " 'hope',\n",
       " 'enjoy',\n",
       " 'happy',\n",
       " 'friday',\n",
       " 'lwwf']"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "clean_tweets(pos_tweets[6])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"We don't like to keep our lovely customers waiting for long! We hope you enjoy! Happy Friday! - LWWF :) https://t.co/smyYriipxI\""
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pos_tweets[6]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['like', 'keep', 'lovely', 'customers', 'waiting', 'long', 'hope', 'enjoy', 'happy', 'friday', 'lwwf']\n"
     ]
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "vec = CountVectorizer(binary=True)\n",
    "vec.fit(clean_tweets(pos_tweets[6]))\n",
    "print([w for w in vec.vocabulary_.keys()])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>like</th>\n",
       "      <th>keep</th>\n",
       "      <th>lovely</th>\n",
       "      <th>customers</th>\n",
       "      <th>waiting</th>\n",
       "      <th>long</th>\n",
       "      <th>hope</th>\n",
       "      <th>enjoy</th>\n",
       "      <th>happy</th>\n",
       "      <th>friday</th>\n",
       "      <th>lwwf</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    like  keep  lovely  customers  waiting  long  hope  enjoy  happy  friday  \\\n",
       "0      0     0       0          0        0     0     1      0      0       0   \n",
       "1      0     0       0          0        0     1     0      0      0       0   \n",
       "2      0     0       0          0        0     0     0      0      1       0   \n",
       "3      1     0       0          0        0     0     0      0      0       0   \n",
       "4      0     0       0          0        0     0     0      0      0       0   \n",
       "5      0     0       0          0        0     0     0      1      0       0   \n",
       "6      0     0       0          0        1     0     0      0      0       0   \n",
       "7      0     1       0          0        0     0     0      0      0       0   \n",
       "8      0     0       0          1        0     0     0      0      0       0   \n",
       "9      0     0       1          0        0     0     0      0      0       0   \n",
       "10     0     0       0          0        0     0     0      0      0       1   \n",
       "\n",
       "    lwwf  \n",
       "0      0  \n",
       "1      0  \n",
       "2      0  \n",
       "3      0  \n",
       "4      1  \n",
       "5      0  \n",
       "6      0  \n",
       "7      0  \n",
       "8      0  \n",
       "9      0  \n",
       "10     0  "
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "pd.DataFrame(vec.transform(clean_tweets(pos_tweets[6])).toarray(), columns=vec.vocabulary_.keys())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5000 5000\n"
     ]
    }
   ],
   "source": [
    "def bag_of_words(tweet):\n",
    "    words = clean_tweets(tweet)\n",
    "    words_dictionary = dict([word, 1] for word in words)    \n",
    "    return words_dictionary\n",
    "pos_tweets_set = []\n",
    "for tweet in pos_tweets:\n",
    "    pos_tweets_set.append((bag_of_words(tweet), 'positive tweet'))    \n",
    " \n",
    "neg_tweets_set = []\n",
    "for tweet in neg_tweets:\n",
    "    \n",
    "    neg_tweets_set.append((bag_of_words(tweet), 'negative tweet'))\n",
    "print (len(pos_tweets_set), len(neg_tweets_set))    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2000 8000\n"
     ]
    }
   ],
   "source": [
    "from random import shuffle \n",
    "shuffle(pos_tweets_set)\n",
    "shuffle(neg_tweets_set)\n",
    " \n",
    "test_set = pos_tweets_set[:1000] + neg_tweets_set[:1000]\n",
    "train_set = pos_tweets_set[1000:] + neg_tweets_set[1000:]\n",
    " \n",
    "print(len(test_set),  len(train_set))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.776\n"
     ]
    }
   ],
   "source": [
    "from nltk import classify\n",
    "from nltk import NaiveBayesClassifier\n",
    " \n",
    "classifier = NaiveBayesClassifier.train(train_set)\n",
    " \n",
    "accuracy = classify.accuracy(classifier, test_set)\n",
    "print(accuracy)\n",
    " \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "positive tweet\n"
     ]
    }
   ],
   "source": [
    "new_tweet = \"the filme was amazing.it was good\"\n",
    "tweet_set = bag_of_words(new_tweet)\n",
    "print (classifier.classify(tweet_set))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "negative tweet\n"
     ]
    }
   ],
   "source": [
    "new_tweet = \"the filme was boring.it was bad\"\n",
    "tweet_set = bag_of_words(new_tweet)\n",
    "print (classifier.classify(tweet_set))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "               |   n   p |\n",
      "               |   e   o |\n",
      "               |   g   s |\n",
      "               |   a   i |\n",
      "               |   t   t |\n",
      "               |   i   i |\n",
      "               |   v   v |\n",
      "               |   e   e |\n",
      "               |         |\n",
      "               |   t   t |\n",
      "               |   w   w |\n",
      "               |   e   e |\n",
      "               |   e   e |\n",
      "               |   t   t |\n",
      "---------------+---------+\n",
      "negative tweet |<758>242 |\n",
      "positive tweet | 206<794>|\n",
      "---------------+---------+\n",
      "(row = reference; col = test)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from collections import defaultdict\n",
    " \n",
    "actual_set = defaultdict(set)\n",
    "predicted_set = defaultdict(set)\n",
    " \n",
    "actual_set_cm = []\n",
    "predicted_set_cm = []\n",
    " \n",
    "for i, (feature, actual_label) in enumerate(test_set):\n",
    "    actual_set[actual_label].add(index)\n",
    "    actual_set_cm.append(actual_label)\n",
    " \n",
    "    predicted_label = classifier.classify(feature)\n",
    " \n",
    "    predicted_set[predicted_label].add(index)\n",
    "    predicted_set_cm.append(predicted_label)\n",
    "    \n",
    "from nltk.metrics import ConfusionMatrix\n",
    "cm = ConfusionMatrix(actual_set_cm, predicted_set_cm)\n",
    "print (cm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "               |      n      p |\n",
      "               |      e      o |\n",
      "               |      g      s |\n",
      "               |      a      i |\n",
      "               |      t      t |\n",
      "               |      i      i |\n",
      "               |      v      v |\n",
      "               |      e      e |\n",
      "               |               |\n",
      "               |      t      t |\n",
      "               |      w      w |\n",
      "               |      e      e |\n",
      "               |      e      e |\n",
      "               |      t      t |\n",
      "---------------+---------------+\n",
      "negative tweet | <37.9%> 12.1% |\n",
      "positive tweet |  10.3% <39.7%>|\n",
      "---------------+---------------+\n",
      "(row = reference; col = test)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print (cm.pretty_format(sort_by_count=True, show_percents=True))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
